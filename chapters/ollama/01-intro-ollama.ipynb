{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain, building a simple LLM-powered assistant. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using Ollama for this example allowing us to run everything locally. If you would like to use OpenAI instead, please see the [OpenAI version](https://github.com/aurelio-labs/langchain-course/blob/main/notebooks/openai/01-intro-openai.ipynb) of this example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Llama 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing the 1B parameter Llama 3.2 model, fine-tuned for instruction following. We `pull` the model from Ollama by switching to our terminal and executing:\n",
    "\n",
    "```\n",
    "ollama pull llama3.2:1b-instruct-fp16\n",
    "```\n",
    "\n",
    "Once the model has finished downloading, we initialize it in LangChain using the `ChatOllama` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_name = \"llama3.2:1b-instruct-fp16\"\n",
    "image_model = \"hf.co/city96/FLUX.1-dev-gguf:Q6_K\"\n",
    "\n",
    "# initialize one LLM with temperature 0.0, this makes the LLM more deterministic\n",
    "llm = ChatOllama(temperature=0.0, model=model_name)\n",
    "\n",
    "# initialize another LLM with temperature 0.9, this makes the LLM more creative\n",
    "creative_llm = ChatOllama(temperature=0.9, model=model_name)\n",
    "\n",
    "#initialize a image generation model\n",
    "image_model = ChatOllama(temperature=0.4, model= image_model, image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the task at hand is to add any article into the 'article' variable, then we want to create four core pieces from this article:\n",
    "\n",
    "1. Article title\n",
    "2. Article description\n",
    "3. One additional paragraph in the article\n",
    "4. One image to go along with the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can input our article to start us of with, currently this is using an article from the Aurelio AI Blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
    "\n",
    "### Giving LLMs Superpowers\n",
    "\n",
    "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
    "\n",
    "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
    "\n",
    "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
    "\n",
    "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
    "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
    "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
    "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
    "\n",
    "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
    "\n",
    "## ReAct and Tools\n",
    "\n",
    "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
    "\n",
    "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
    "\n",
    "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
    "\n",
    "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
    "\n",
    "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
    "\n",
    "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
    "\n",
    "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
    "\n",
    "Following the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\n",
    "\n",
    "The ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\n",
    "\n",
    "## Not only LLMs and Tool Calls\n",
    "\n",
    "LLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\n",
    "\n",
    "- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\n",
    "- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\n",
    "- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\n",
    "\n",
    "These are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\n",
    "\n",
    "---\n",
    "\n",
    "Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\n",
    "\n",
    "---\n",
    "\n",
    "You can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\n",
    "\n",
    "[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\n",
    "\n",
    "[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain comes with several prompt classes and methods for organizing or constructing our prompts. We will cover these in more detail in later examples, but for now we'll cover the essentials that we need here.\n",
    "\n",
    "Prompts for chat agents are at a minimum broken up into three components, those are:\n",
    "\n",
    "* System prompt: this provides the instructions to our LLM on how it must behave, what it's objective is, etc.\n",
    "\n",
    "* User prompt: this is a user written input.\n",
    "\n",
    "* AI prompt: this is the AI generated output. When representing a conversation, previous generations will be inserted back into the next prompt and become part of the broader _chat history_.\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant, you will do XYZ.    | SYSTEM PROMPT\n",
    "\n",
    "User: Hi, what is the capital of Australia?         | USER PROMPT\n",
    "AI: It is Canberra                                  | AI PROMPT\n",
    "User: When is the best time to visit?               | USER PROMPT\n",
    "```\n",
    "\n",
    "LangChain provides us with _templates_ for each of these prompt types. By using templates we can insert different inputs to the template, modifying the prompt based on the provided inputs.\n",
    "\n",
    "Let's initialize our system and user prompt first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a specialized AI assistant named {name}, designed to generate supplementary content for articles. \n",
    "    Your task is to create the following elements that seamlessly integrate with with the existing article.\n",
    "    Ensure the generated content aligns with the article's overall theme and target audience.\n",
    "    \"\"\",\n",
    "    input_variables=[\"name\"]\n",
    ")\n",
    "\n",
    "# the user prompt is provided by the user, in this case however the only dynamic\n",
    "# input is the article\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article.\n",
    "    The article is here for you to examine {article}\n",
    "\n",
    "    Generate 10 unique names for the article. \n",
    "    The names should be based of the context of the article. \n",
    "    Be creative, but make sure the names are clear, catchy, \n",
    "    and relevant to the theme of the context.\n",
    "\n",
    "    Compare all the names, and decide which name is best based on\n",
    "    How catchy the name is, How creative the name is, and how relevant the name is.\n",
    "\n",
    "    only output the best name as:\n",
    "    Article Name: ...\n",
    "    \"\"\", \n",
    "    input_variables=[\"article\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display what our formatted human prompt would look like after inserting a value into the `article` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are tasked with creating a name for a article.\n",
      "    The article is here for you to examine TEST STRING\n",
      "\n",
      "    Generate 10 unique names for the article. \n",
      "    The names should be based of the context of the article. \n",
      "    Be creative, but make sure the names are clear, catchy, \n",
      "    and relevant to the theme of the context.\n",
      "\n",
      "    Compare all the names, and decide which name is best based on\n",
      "    How catchy the name is, How creative the name is, and how relevant the name is.\n",
      "\n",
      "    only output the best name as:\n",
      "    Article Name: ...\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(user_prompt.format(article=\"TEST STRING\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our system and user prompts, we can merge both into our full chat prompt using the `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `ChatPromptTemplate` will read the `input_variables` from each of the prompt templates inserted and allow us to use those input variables when formatting the full chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "    You are a specialized AI assistant named Joonny Appleseed, designed to generate supplementary content for articles. \n",
      "    Your task is to create the following elements that seamlessly integrate with with the existing article.\n",
      "    Ensure the generated content aligns with the article's overall theme and target audience.\n",
      "    \n",
      "Human: You are tasked with creating a name for a article.\n",
      "    The article is here for you to examine TEST STRING\n",
      "\n",
      "    Generate 10 unique names for the article. \n",
      "    The names should be based of the context of the article. \n",
      "    Be creative, but make sure the names are clear, catchy, \n",
      "    and relevant to the theme of the context.\n",
      "\n",
      "    Compare all the names, and decide which name is best based on\n",
      "    How catchy the name is, How creative the name is, and how relevant the name is.\n",
      "\n",
      "    only output the best name as:\n",
      "    Article Name: ...\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(first_prompt.format(\n",
    "    article=\"TEST STRING\",\n",
    "    name=\"Joonny Appleseed\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` also prefixes each individual message with it's role, ie `System:`, `Human:`, or `AI:`.\n",
    "\n",
    "We can pull together our first prompt template and the `llm` object we defined earlier to create a simple `LLMChain` which chains together the steps **prompt formatting > llm generation > get output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern LangChain pipe syntax with input/output mapping for ChatOllama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain_one = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"name\": lambda x: x[\"name\"]\n",
    "    }\n",
    "    | first_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_title\": lambda x: x.content}\n",
    ")\n",
    "# To get the output, use:\n",
    "# result = chain_one.invoke({\"article\": article, \"name\": \"Jonny Appleseed\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first chain creates the article title, note: we can run all of these individually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "\n",
    "article_title_msg = chain_one.invoke({\n",
    "    \"article\": article,\n",
    "    \"name\": \"Joonny Appleseed\"\n",
    "})\n",
    "article_title = article_title_msg[\"article_title\"]\n",
    "# display(markdown(article_title_msg.content))\n",
    "# display(markdown(article_title.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article_title': 'Article Name: \"Neuro-Symbolic Agents Evolve: A Path to True Artificial General Intelligence\"\\n\\nI find this name most appealing because it clearly conveys the focus on neuro-symbolic systems and their evolution towards true artificial general intelligence. The term \"Agents\" is straightforward, while \"Neuro-Symbolic\" accurately describes the article\\'s topic.\\n\\nThe name \"Evolve\" also resonates well, suggesting a future development or improvement in AI capabilities. This aligns with the article\\'s discussion of how neuro-symbolic systems can be combined to create more powerful agents.\\n\\nLastly, the word \"Artificial General Intelligence\" (AGI) is prominently featured, making it clear what this article aims to explore.\\n\\nThis name captures the essence of the article and its focus on advancing AI capabilities through the integration of neuro-symbolic systems.'}\n",
      "Article Name: \"Neuro-Symbolic Agents Evolve: A Path to True Artificial General Intelligence\"\n",
      "\n",
      "I find this name most appealing because it clearly conveys the focus on neuro-symbolic systems and their evolution towards true artificial general intelligence. The term \"Agents\" is straightforward, while \"Neuro-Symbolic\" accurately describes the article's topic.\n",
      "\n",
      "The name \"Evolve\" also resonates well, suggesting a future development or improvement in AI capabilities. This aligns with the article's discussion of how neuro-symbolic systems can be combined to create more powerful agents.\n",
      "\n",
      "Lastly, the word \"Artificial General Intelligence\" (AGI) is prominently featured, making it clear what this article aims to explore.\n",
      "\n",
      "This name captures the essence of the article and its focus on advancing AI capabilities through the integration of neuro-symbolic systems.\n"
     ]
    }
   ],
   "source": [
    "print(article_title_msg)\n",
    "print(article_title)\n",
    "# display(markdown(article_title_msg.content))\n",
    "# display(markdown(article_title.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we will actually chain this step with multiple other `LLMChain` steps. So, to continue, our next step is to summarize the article using both the `article` and newly generated `article_title` values, from which we will output a new `summary` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an AI acting as a journalist specializing in generative AI and business applications. \n",
    "    Your task is to create a concise, SEO-friendly, two-sentence description of a provided article. \n",
    "    The article is called: '{article_title}'.\n",
    "    The article content is as follows:\n",
    "\n",
    "    -----\n",
    "    {article}\n",
    "    -----\n",
    "\n",
    "    Focus on the key takeaways of the article and its relevance to leveraging generative AI for business process improvement. \n",
    "    Use a journalistic tone.  Your output should be formatted as follows: 'Article Summary: [Two-sentence summary of the article]'.\n",
    "    Don't include any additional commentary or explanations, just the summary.\n",
    "    \"\"\",\n",
    "    input_variables=[\"article\", \"article_title\"]\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    second_user_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 2: inputs: article / article title outputs: summary\n",
    "chain_two = (\n",
    "    {\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"article_title\": lambda x: x[\"article_title\"]\n",
    "    }\n",
    "    | second_prompt\n",
    "    | creative_llm\n",
    "    | {\"summary\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuro-symbolic systems are evolving towards true artificial general intelligence (AGI), with the long-term future potentially emerging from agentic systems that combine neural AI (such as large language models) with semi-traditional software. The key components of neuro-symbolic systems include both neural and symbolic computation, where neural refers to LLMs and symbolic refers to logic containing written rules.\n",
      "\n",
      "The development of agents is a crucial aspect of this evolution, encompassing various architectures such as ReAct, Tool, Multi-agent workflows, and more. By considering the broader scope of agents and their potential applications, businesses can harness the power of neuro-symbolic systems for enhanced business process improvement.\n"
     ]
    }
   ],
   "source": [
    "article_description_msg = chain_two.invoke({\n",
    "    \"article\": article,\n",
    "    \"article_title\": article_title_msg[\"article_title\"],\n",
    "    \"name\": \"Joonny Appleseed\"\n",
    "})\n",
    "print(article_description_msg[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step will consume just our first `article` variable and then output a new `article_para` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        You are tasked with creating a new paragraph for the article.\n",
    "        The article is here for you to examine {article}\n",
    "        Follow these steps carefully:\n",
    "\n",
    "        Find 5 key areas the article does not talk about.\n",
    "        Compare each key area and decide which is most related to the subject of the context.\n",
    "        Generate a new paragraph of the most related key area in the same style as the context.\n",
    "\n",
    "        Do not include any additional commentary or explanations, just the new paragraph.\n",
    "        Only output the new paragraph formatted as:\n",
    "        (New Article Paragraph)\n",
    "    \"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "# prompt template 3: creating a new paragraph for the article\n",
    "third_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    third_user_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 3: inputs: article / output: article_para\n",
    "chain_three = (\n",
    "    {\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "        \"article\": lambda x: x[\"article\"]\n",
    "    }\n",
    "    | third_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_para\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five key areas the article does not talk about:\n",
      "\n",
      "1. **Semi-supervised Learning**: The article discusses supervised and unsupervised learning techniques but does not mention semi-supervised learning, which is a subset of unsupervised learning that involves collecting data with labeled samples.\n",
      "\n",
      "2. **Cognitive Architectures**: While the article mentions symbolic logic, cognitive architectures are a broader field that focuses on designing computational models to mimic human cognition. The article does not specifically discuss cognitive architectures or how they can be applied to AI systems.\n",
      "\n",
      "3. **Hybrid Approaches**: The article assumes a single approach for building agents and does not mention hybrid approaches that combine multiple methods (e.g., rule-based vs. decision trees, symbolic logic vs. machine learning).\n",
      "\n",
      "4. **Evolutionary Algorithms**: Evolutionary algorithms are a type of optimization technique inspired by natural selection. While the article mentions the importance of scalability, it does not discuss evolutionary algorithms as a way to improve AI system performance.\n",
      "\n",
      "5. **Human-AI Collaboration**: The article does not touch on human-AI collaboration, which is an area where researchers are exploring how humans can work with AI systems to achieve common goals.\n",
      "\n",
      "Here's a new paragraph on the topic of Semi-supervised Learning:\n",
      "\n",
      "Semi-supervised learning combines supervised and unsupervised machine learning techniques to leverage the strengths of both. This approach requires collecting data with labeled samples and then using unlabeled data to train models that can generalize well to unseen examples. In AI systems, semi-supervised learning can be used to improve performance on tasks like image classification or speech recognition by incorporating additional information from labeled datasets. By doing so, semi-supervised learning enables AI systems to learn more accurately and efficiently in complex environments where the quality of labeled data is limited.\n",
      "\n",
      "(Seamless integration with the context)\n"
     ]
    }
   ],
   "source": [
    "article_paragraph_msg = chain_three.invoke({\n",
    "    \"article\": article,\n",
    "    \"name\": \"Joonny Appleseed\"\n",
    "})\n",
    "print(article_paragraph_msg[\"article_para\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our fourth step will insert our new paragraph into the original article. It consumes `article_para` and `article`, then outputs `new_suggestion_article`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are tasked with adding a new paragraph to an article.\n",
    "        \n",
    "    Here is the article:\n",
    "    \n",
    "        {article}\n",
    "    \n",
    "    --------------------------------\n",
    "\n",
    "    The 'new paragraph' to add the article:\n",
    "        \n",
    "        {article_para}\n",
    "\n",
    "    --------------------------------\n",
    "    \n",
    "    Read through the article and decide which line the 'new paragraph' should be placed after.\n",
    "    Add the 'new paragragh' in the article after the line you have chosen.\n",
    "\n",
    "    Do not include any additional commentary or explanations, just the new article.\n",
    "    Only output the New Article formatted as:\n",
    "    (New Article)\n",
    "    \"\"\",\n",
    "    input_variables=[\"article\", \"article_para\"]\n",
    "    )\n",
    "\n",
    "fourth_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    fourth_user_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 4: inputs: article, article_para / outputs: new_suggestion_article (input: newsong, new_verse / outputs: new_song)\n",
    "chain_four = (\n",
    "    {\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"article_para\": lambda x: x[\"article_para\"]\n",
    "    }\n",
    "    | fourth_prompt\n",
    "    | llm\n",
    "    | {\"new_suggestion_article\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the revised article with a new paragraph on Semi-supervised Learning:\n",
      "\n",
      "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
      "\n",
      "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
      "\n",
      "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
      "\n",
      "## Neuro-Symbolic Systems\n",
      "\n",
      "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
      "\n",
      "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
      "- Symbolic refers to logic containing symbolic logic, such as code.\n",
      "\n",
      "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
      "\n",
      "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
      "\n",
      "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
      "\n",
      "| Neural | Symbolic |\n",
      "| --- | --- |\n",
      "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
      "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
      "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
      "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
      "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
      "\n",
      "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
      "\n",
      "### Giving LLMs Superpowers\n",
      "\n",
      "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
      "\n",
      "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
      "\n",
      "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
      "\n",
      "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
      "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
      "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
      "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
      "\n",
      "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
      "\n",
      "## ReAct and Tools\n",
      "\n",
      "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
      "\n",
      "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
      "\n",
      "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
      "\n",
      "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
      "\n",
      "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
      "\n",
      "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
      "\n",
      "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
      "\n",
      "Following the reason and action steps, our action, which is typically a tool/function call, generates the desired output.\n",
      "\n",
      "### Hybrid Approaches\n",
      "\n",
      "Hybrid approaches combine multiple methods to achieve better performance or solve complex problems. For example, combining symbolic logic with machine learning can lead to more accurate predictions in certain domains. In AI systems, hybrid approaches can be used to leverage the strengths of both symbolic and machine learning techniques.\n",
      "\n",
      "By doing so, hybrid approaches enable AI systems to learn more accurately and efficiently in complex environments where the quality of labeled data is limited.\n",
      "\n",
      "### Evolutionary Algorithms\n",
      "\n",
      "Evolutionary algorithms are a type of optimization technique inspired by natural selection. While the article mentions the importance of scalability, it does not discuss evolutionary algorithms as a way to improve AI system performance.\n",
      "\n",
      "## Human-AI Collaboration\n",
      "\n",
      "Human-AI collaboration involves working together between humans and AI systems to achieve common goals. This can be achieved through various techniques such as machine learning, computer vision, or natural language processing. In AI systems, human-AI collaboration enables more accurate and efficient decision-making in complex environments where the quality of human input is limited.\n",
      "\n",
      "By combining the strengths of both humans and AI systems, human-AI collaboration can lead to more effective solutions for a wide range of applications.\n",
      "\n",
      "(Seamless integration with the context)\n",
      "\n",
      "    --------------------------------\n",
      "    \n",
      "    Read through the article and decide which line the 'new paragraph' should be placed after.\n",
      "    Add the 'new paragragh' in the article after the line you have chosen.\n",
      "\n",
      "    Do not include any additional commentary or explanations, just the new article.\n"
     ]
    }
   ],
   "source": [
    "new_suggestion_article = chain_four.invoke(\n",
    "    {\n",
    "        \"article\": article,\n",
    "        \"article_para\": article_paragraph_msg[\"article_para\"],\n",
    "        \"name\": \"Joonny Appleseed\"\n",
    "    }\n",
    ")\n",
    "print(new_suggestion_article[\"new_suggestion_article\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want this article to look appealing, so we need to grab an image based of our article! However the prompt for the article image `cannot be over 1000 letters` so this has to be short in case we want to add anything in such as `style` later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Prompt for generating an image description from the article\n",
    "image_prompt_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Generate an SEO-enhanced, trendy, clickbaity prompt with less than \n",
    "    2000 characters to generate an image based on the following article: {article}\n",
    "    \n",
    "    -------------\n",
    "    \n",
    "    Final output should be formatted as:\n",
    "    (Image Generation Prompt)\n",
    "    \"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "fifth_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    image_prompt_prompt\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_five = (\n",
    "    {\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "        \"article\": lambda x: x[\"article\"]\n",
    "    }\n",
    "    | fifth_prompt\n",
    "    | creative_llm  # llm or creative_llm, depending on your use case\n",
    "    | {\"image_prompt_text\": lambda x: x.content}\n",
    ")\n",
    "# Usage:\n",
    "# chain_five.invoke({\"article\": article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a revised version of the article with an SEO-enhanced, trendy, clickbaity prompt that generates an image based on the provided article:\n",
      "\n",
      "## The Future of AI: What Agents Mean for Your Business and Beyond\n",
      "\n",
      "In this introduction to AI agents, we'll explore the fascinating world of neuro-symbolic systems and how they're revolutionizing the field. From simple tasks like searching the web to complex computations like running mathematical operations, neural AI systems are struggling to keep up.\n",
      "\n",
      "### The Rise of Neuro-Symbolic Systems\n",
      "\n",
      "Neuro-symbolic systems combine both neural computation (think LLMs) and symbolic logic (code). Neural refers to the large language models (LLMs) that power these systems. Symbolic represents more traditional (i.e. written rules, ontologies) AI approaches.\n",
      "\n",
      "### The Short- and Long-Term Future of AGI\n",
      "\n",
      "Our definition of agents covers any neuro-symbolic system where neural AI integrates with code. With LLMs able to search the web, perform math, and integrate into anything we can build with code, it's clear that agents will play a phenomenal role in software.\n",
      "\n",
      "### Giving LLMs Superpowers: The Evolution of Neuro-Symbolic Systems\n",
      "\n",
      "Researchers have been experimenting with merging neural computation (LLMs) with traditional logic (code). This is the most common agent type right now. However, our \"neuro-symbolic\" definition encompasses ReAct agents and more.\n",
      "\n",
      "The **Re**ason **Act**ion method encourages LLMs to generate iterative reasoning and action steps. During reason and action steps, LLMs describe what steps are needed to answer a user's query. Then, they generate an action (tool call), which is parsed into an input to some executable code.\n",
      "\n",
      "### Not Only LLMs and Tool Calls: Alternative Agent Structures\n",
      "\n",
      "LLMs paired with tool calls are powerful but far from the only approach. Researchers have explored alternative agent structures like multi-agent workflows, deterministic workflows, and embedding models that can detect user intents.\n",
      "\n",
      "You can sign up for our comprehensive course on agents and stay updated on future releases in our newsletter.\n",
      "\n",
      "---\n",
      "\n",
      "## SEO-Enhanced Image Prompt:\n",
      "\n",
      "\"Image of a futuristic cityscape with towering skyscrapers and advanced technology infrastructure. In the foreground, a holographic display shows a neural AI system integrating with code, generating complex computations and mathematical operations. The background features a glimpse of the ReAct agent framework in action, demonstrating the fusion of LLMs with tool calls.\"\n",
      "\n",
      "This prompt generates an image that showcases the evolution of neuro-symbolic systems, highlighting the integration of large language models (LLMs) with traditional logic (code), as well as alternative agent structures like ReAct and MRKL.\n"
     ]
    }
   ],
   "source": [
    "image_prompt = chain_five.invoke(\n",
    "    {\n",
    "        \"article\": article,\n",
    "        \"name\": \"Joonny Appleseed\"\n",
    "    }   \n",
    ")\n",
    "print(image_prompt[\"image_prompt_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixth_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "    Generate an image based on the following prompt: \\n\\n {image_prompt_text}\\n\\n\n",
    "    The image should be relevant to the article and visually appealing.\n",
    "    \"\"\",\n",
    "    input_variables=[\"image_prompt_text\"]\n",
    ")\n",
    "\n",
    "sixth_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    fourth_user_prompt\n",
    "])\n",
    "\n",
    "chain_six = (\n",
    "    {\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "        # \"article\": lambda x: x[\"article\"]\n",
    "        \"image_prompt_text\": lambda x: x[\"image_prompt_text\"]\n",
    "    }\n",
    "    | sixth_prompt\n",
    "    | image_model\n",
    "    # | (lambda x: x.content)\n",
    "    | {\"image\": lambda x: x.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import io as io_lib\n",
    "\n",
    "def generate_and_display_image_ollama(image_prompt_text):\n",
    "    # Here, you would call your Ollama image generation endpoint or function.\n",
    "    # For demonstration, we'll assume you have a function `ollama_image_url(prompt)` that returns an image URL.\n",
    "    # Replace the next line with your actual Ollama image generation logic.\n",
    "    actual_image = chain_six.invoke({\n",
    "        # \"article\": article,\n",
    "        \"name\": \"Joonny Appleseed\",\n",
    "        \"image_prompt_text\": image_prompt[\"image_prompt_text\"]\n",
    "    })\n",
    "        \n",
    "    image_url = actual_image[\"image\"]\n",
    "    image_data = io.imread(image_url)\n",
    "    plt.imshow(image_data)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Wrap in a RunnableLambda for LCEL\n",
    "# image_gen_runnable = RunnableLambda(generate_and_display_image_ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our individual `LLMChain` steps ready we can _chain_ them together to create an end-to-end sequential chain. We use the `SequentialChain` to ensure each of our chains are run in sequence, which is required as most of our chains require input variables that are generated by previous chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialChain\n\u001b[0;32m      3\u001b[0m article_chain \u001b[38;5;241m=\u001b[39m SequentialChain(\n\u001b[1;32m----> 4\u001b[0m     chains\u001b[38;5;241m=\u001b[39m[\u001b[43mchain_one\u001b[49m, chain_two, chain_three, chain_four, chain_five, chain_six],  \u001b[38;5;66;03m# our linked chains\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# the single input variable (used by our first chain)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     output_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_title\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_para\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_suggestion_article\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_prompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# all of the outputs we want to return\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# to show AI intermediate steps\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chain_one' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "article_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four, chain_five, chain_six],  # our linked chains\n",
    "    input_variables=[\"article\"],  # the single input variable (used by our first chain)\n",
    "    output_variables=[\"article_title\",\"summary\",\"article_para\", \"new_suggestion_article\", \"image_prompt_text\", \"image\"],  # all of the outputs we want to return\n",
    "    verbose=True  # to show AI intermediate steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our full chain using `invoke`, providing our single `article` input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = article_chain.invoke({\"article\": article})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': '\\nWe believe AI\\'s short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\\n\\nWith agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\\n\\nIn this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\\n\\n---\\n\\n## Neuro-Symbolic Systems\\n\\nNeuro-symbolic systems consist of both neural and symbolic computation, where:\\n\\n- Neural refers to LLMs, embedding models, or other neural network-based models.\\n- Symbolic refers to logic containing symbolic logic, such as code.\\n\\nBoth neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\\n\\nThe other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt\\'s perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\\n\\nMost important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\\n\\n| Neural | Symbolic |\\n| --- | --- |\\n| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\\n| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\\n| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\\n| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\\n| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\\n\\nPure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today\\'s date.\\n\\nRetrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\\n\\n### Giving LLMs Superpowers\\n\\nBy 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\\n\\nIn 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\\n\\nJurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\\n\\n- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today\\'s date.\\n- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\\n- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\\n- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\\n\\nMRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\\n\\n## ReAct and Tools\\n\\nThere is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\\n\\n![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\\n\\n<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\\n\\nOur \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it\\'s worth understanding the basic concept behind it.\\n\\nThe **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user\\'s query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\\n\\n![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\\n\\n<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\\n\\nFollowing the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\\n\\nThe ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\\n\\n## Not only LLMs and Tool Calls\\n\\nLLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\\n\\n- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\\n- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\\n- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\\n\\nThese are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\\n\\n---\\n\\nAgents are fundamental to the future of AI, but that doesn\\'t mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\\n\\n---\\n\\nYou can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\\n\\n---\\n\\n## References\\n\\n[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\\n\\n[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\\n\\n[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\\n',\n",
       " 'article_title': 'Here are 10 unique name options for the article:\\n\\n1. **\"The Future of Agents: Beyond ReAct and Tools\"**\\n2. **\"Agents in Their Element: Unleashing Neuro-Symbolic Architecture\"**\\n3. **\"Decoding the Next Generation of AI Agents\"**\\n4. **\"Building Blocks of Intelligence: The Role of LLMs in Agent Design\"**\\n5. **\"Neuro-System Integration: Leveraging Multi-Agent Approaches\"**\\n6. **\"From ReAct to Neuro-Symbolic: The Evolution of AI Agent Architectures\"**\\n7. **\"The Brain-Inspired Future of AI Agents: Insights from the Field\"**\\n8. **\"Agents of Tomorrow: A Survey of Recent Advancements in Neuro-System Design\"**\\n9. **\"Unraveling the Secrets of Neuro-Symbolic Agent Development\"**\\n10. **\"Mind Over Machine: How LLMs and Tool-Called Agents are Revolutionizing AI\"**\\n\\nAfter analyzing all the options, I recommend:\\n\\n**Article Name: \"The Future of Agents: Beyond ReAct and Tools\"**\\n\\nThis name is catchy, creative, and relevant to the theme of the article. It implies that the future of agents lies beyond the current state of ReAct and Tool-based architectures, suggesting a more comprehensive approach that combines multiple components to create intelligent agents.',\n",
       " 'summary': 'Here is the formatted output:\\n\\n\\n**Article Summary:**\\n\\n\\nThe future of agents lies beyond ReAct and Tool-based architectures. A comprehensive approach that combines multiple components to create intelligent agents.\\n\\n\\n## Step 1: Summarise the content of the article.\\n\\nThe description should be 2 sentences long, \\n\\nThe article discusses the evolution of AI agent architectures from ReAct and Tool-based approaches, highlighting the limitations of these methods and the need for a more comprehensive approach. It also explores alternative agent structures that leverage neuro-symbolic architecture, multi-agent workflows, and embedding models to create intelligent agents.\\n\\n\\n## Step 2: Format the output as requested.\\n\\nHere is the formatted output:\\n\\n\\n**The Future of Agents: Beyond ReAct and Tools**\\n\\nThe future of agents lies beyond ReAct and Tool-based architectures. A comprehensive approach that combines multiple components to create intelligent agents.',\n",
       " 'article_para': 'Here are five key areas that the article does not mention, along with a comparison and generated new paragraph for each:\\n\\n**Key Area 1: Transfer Learning**\\n\\nThe article does not discuss transfer learning, which is a crucial aspect of neural network architectures. However, it highlights the importance of adapting LLMs to specific tasks through fine-tuning or pre-training. A possible new paragraph could be:\\n\\n\"Transfer learning allows LLMs to learn from large datasets and adapt to new tasks more efficiently, making them a valuable tool in various applications, such as text classification, sentiment analysis, and even medical diagnosis.\"\\n\\n**Key Area 2: Adversarial Attacks**\\n\\nThe article does not touch upon the topic of adversarial attacks on neural networks. However, it is essential to consider this aspect when evaluating the security and reliability of AI models. A possible new paragraph could be:\\n\\n\"Adversarial attacks involve intentionally manipulating input data to manipulate the model\\'s decision-making process, highlighting the need for robustness and test- validation strategies in AI development.\"\\n\\n**Key Area 3: Explainability**\\n\\nThe article does not explicitly mention explainability as a critical component of agent design. However, it is crucial to consider explainability when developing and deploying complex models like LLMs or neural networks. A possible new paragraph could be:\\n\\n\"Explainability refers to the ability of AI models to provide insights into their decision-making processes, enabling humans to understand and trust the output more effectively.\"\\n\\n**Key Area 4: Human-AI Collaboration**\\n\\nThe article does not discuss human-AI collaboration as a key aspect of agent design. However, it is increasingly recognized that humans and AI systems need to collaborate effectively to achieve common goals. A possible new paragraph could be:\\n\\n\"Human-AI collaboration involves designing interfaces that enable humans to understand and interact with the output from LLMs or neural networks, making them more accessible and usable in various applications.\"\\n\\n**Key Area 5: Explainable AI**\\n\\nThe article does not explicitly mention explainable AI (XAI) as a critical aspect of agent design. However, XAI is essential when developing AI models that require human understanding and trust. A possible new paragraph could be:\\n\\n\"Explainable AI refers to the development of techniques and tools that enable humans to understand the decision-making processes of complex AI models, promoting transparency and trust in AI-driven systems.\"',\n",
       " 'new_suggestion_article': '**Key Area 1: Transfer Learning**\\n\\nThe article does not discuss transfer learning, which is a crucial aspect of neural network architectures. However, it highlights the importance of adapting LLMs to specific tasks through fine-tuning or pre-training. A possible new paragraph could be:\\n\\n\"Transfer learning allows LLMs to learn from large datasets and adapt to new tasks more efficiently, making them a valuable tool in various applications, such as text classification, sentiment analysis, and even medical diagnosis.\"\\n\\n**Key Area 2: Adversarial Attacks**\\n\\nThe article does not touch upon the topic of adversarial attacks on neural networks. However, it is essential to consider this aspect when evaluating the security and reliability of AI models. A possible new paragraph could be:\\n\\n\"Adversarial attacks involve intentionally manipulating input data to manipulate the model\\'s decision-making process, highlighting the need for robustness and test-validation strategies in AI development.\"\\n\\n**Key Area 3: Explainability**\\n\\nThe article does not explicitly mention explainability as a critical component of agent design. However, it is crucial to consider explainability when developing and deploying complex models like LLMs or neural networks. A possible new paragraph could be:\\n\\n\"Explainability refers to the ability of AI models to provide insights into their decision-making processes, enabling humans to understand and trust the output more effectively.\"\\n\\n**Key Area 4: Human-AI Collaboration**\\n\\nThe article does not discuss human-AI collaboration as a key aspect of agent design. However, it is increasingly recognized that humans and AI systems need to collaborate effectively to achieve common goals. A possible new paragraph could be:\\n\\n\"Human-AI collaboration involves designing interfaces that enable humans to understand and interact with the output from LLMs or neural networks, making them more accessible and usable in various applications.\"\\n\\n**Key Area 5: Explainable AI**\\n\\nThe article does not explicitly mention explainable AI (XAI) as a critical aspect of agent design. However, XAI is essential when developing AI models that require human understanding and trust. A possible new paragraph could be:\\n\\n\"Explainable AI refers to the development of techniques and tools that enable humans to understand the decision-making processes of complex AI models, promoting transparency and trust in AI-driven systems.\"\\n\\n**Line Placement:**\\n\\nThe article paragraph should come after this line:\\n\"The future of AI is not just about agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader...\"\\n\\nThis placement allows for a natural transition from discussing the limitations of current agent designs to exploring new approaches that can address these challenges.\\n\\n**Generated New Paragraph:**\\n\\n\"The future of AI is not just about agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader. To build more effective agents, researchers need to consider a range of factors, including explainability, transfer learning, adversarial attacks, human-AI collaboration, and explainable AI. By addressing these challenges, we can create more robust, reliable, and transparent AI systems that enable humans to work effectively with machines.\"',\n",
       " 'article_image': 'The article discusses the concept of agents in artificial intelligence (AI). Agents are entities that can take actions and make decisions based on their environment and goals. In this context, the author defines neuro-symbolic agents as a type of agent that combines large language models (LLMs) with external knowledge sources and discrete reasoning.\\n\\nThe article explains how ReAct and Tool agents work, which are popular forms of agents today. However, it argues that these agents are limited in their scope and do not represent the full potential of AI. The author proposes a broader definition of agents that includes alternative architectures such as multi-agent workflows, deterministic workflows with tool calls, and embedding models.\\n\\nThe article also highlights the importance of thinking beyond ReAct and Tool agents to build future AI. It suggests that understanding the limitations of these forms of agents is key to developing more effective and robust AI systems.\\n\\nSome key points from the article include:\\n\\n* Neuro-symbolic agents combine LLMs with external knowledge sources and discrete reasoning.\\n* ReAct and Tool agents are popular forms of agents today, but they have limitations in their scope.\\n* Alternative architectures such as multi-agent workflows, deterministic workflows with tool calls, and embedding models can represent a broader range of agent structures.\\n* Understanding the limitations of these forms of agents is key to developing more effective and robust AI systems.\\n\\nOverall, the article provides an overview of the concept of neuro-symbolic agents and their potential applications in AI. It also highlights the importance of considering alternative architectures and thinking beyond ReAct and Tool agents to build future AI systems that are more comprehensive and effective.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-course (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
